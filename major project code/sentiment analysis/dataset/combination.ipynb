{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFile(path):\n",
    "    #param path: path/to/file.ext (str)\n",
    "    #Returns contents of file (str)\n",
    "    with open(path) as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "imdb_data = openFile('./dataset/imdb_labelled.txt')\n",
    "amzn_data = openFile('./dataset/amazon_cells_labelled.txt')\n",
    "yelp_data = openFile('./dataset/yelp_labelled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [imdb_data, amzn_data, yelp_data]\n",
    "combined_dataset = []\n",
    "# separate samples from each other\n",
    "for dataset in datasets:\n",
    "    combined_dataset.extend(dataset.split('\\n'))\n",
    "\n",
    "# separate each label from each sample\n",
    "dataset = [sample.split('\\t') for sample in combined_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=dataset, columns=['Reviews', 'Labels'])\n",
    "\n",
    "# Remove any blank reviews\n",
    "df = df[df[\"Labels\"].notnull()]\n",
    "\n",
    "# shuffle the dataset for later.\n",
    "# Note this isn't necessary (the dataset is shuffled again before used), \n",
    "# but is good practice.\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>Maybe it's just their Vegetarian fare, but I'v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>If you like a loud buzzing to override all you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wasted two hours.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>I found this place by accident and I could not...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>If you are Razr owner...you must have this!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Reviews Labels\n",
       "2951  Maybe it's just their Vegetarian fare, but I'v...      0\n",
       "1745  If you like a loud buzzing to override all you...      0\n",
       "6                                   Wasted two hours.        0\n",
       "2034  I found this place by accident and I could not...      1\n",
       "1008        If you are Razr owner...you must have this!      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_review = []\n",
    "negative_review = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every review if its lable is 1 than store in positive review else store in negative review\n",
    "for i in range(len(df)):\n",
    "    if(df.iloc[i][1] == '1'):\n",
    "        positive_review.append(df.iloc[i][0])\n",
    "    else:\n",
    "        negative_review.append(df.iloc[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_map = {} #dict\n",
    "current_index = 0\n",
    "positive_tokenized = [] #array\n",
    "negative_tokenized = [] #array\n",
    "orig_reviews = [] #array\n",
    "stopword_english = stopwords.words('english')\n",
    "important_words = ['above', 'below', 'off', 'over', 'under', 'more', 'most', 'such', 'no', 'nor', 'not', 'only', 'so', 'than', 'too', 'very', 'just', 'but']\n",
    "ENGLISH_STOPWORDS = set(stopword_english) - set(important_words)\n",
    "NEG_CONTRACTIONS = [\n",
    "    (r'aren\\'t', 'are not'),\n",
    "    (r'can\\'t', 'can not'),\n",
    "    (r'couldn\\'t', 'could not'),\n",
    "    (r'daren\\'t', 'dare not'),\n",
    "    (r'didn\\'t', 'did not'),\n",
    "    (r'doesn\\'t', 'does not'),\n",
    "    (r'don\\'t', 'do not'),\n",
    "    (r'isn\\'t', 'is not'),\n",
    "    (r'hasn\\'t', 'has not'),\n",
    "    (r'haven\\'t', 'have not'),\n",
    "    (r'hadn\\'t', 'had not'),\n",
    "    (r'mayn\\'t', 'may not'),\n",
    "    (r'mightn\\'t', 'might not'),\n",
    "    (r'mustn\\'t', 'must not'),\n",
    "    (r'needn\\'t', 'need not'),\n",
    "    (r'oughtn\\'t', 'ought not'),\n",
    "    (r'shan\\'t', 'shall not'),\n",
    "    (r'shouldn\\'t', 'should not'),\n",
    "    (r'wasn\\'t', 'was not'),\n",
    "    (r'weren\\'t', 'were not'),\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'wouldn\\'t', 'would not'),\n",
    "    (r'ain\\'t', 'am not') # not only but stopword anyway\n",
    "]\n",
    "OTHER_CONTRACTIONS = {\n",
    "    \"'m\": 'am',\n",
    "    \"'ll\": 'will',\n",
    "    \"'s\": 'has', # or 'is' but both are stopwords\n",
    "    \"'d\": 'had'  # or 'would' but both are stopwords\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(words):    \n",
    "    words_dictionary = [word for word in words]    \n",
    "    return words_dictionary\n",
    " \n",
    "# feature extractor function for ngrams (bigram)\n",
    "def bag_of_ngrams(words, n=2):\n",
    "    words_ng = []\n",
    "    for item in iter(ngrams(words, n)):\n",
    "        words_ng.append(item)\n",
    "    words_dictionary = [word for word in words_ng]     \n",
    "    return words_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(doc):\n",
    "    all_features = []\n",
    "    doc = doc.lower() # downcase\n",
    "    # transform negative contractions (e.g don't --> do not)\n",
    "    for t in NEG_CONTRACTIONS:\n",
    "        doc = re.sub(t[0], t[1], doc)\n",
    "    tokens = nltk.tokenize.word_tokenize(doc) # split string into words (tokens)\n",
    "    # transform other contractions (e.g 'll --> will)\n",
    "    tokens = [OTHER_CONTRACTIONS[token] if OTHER_CONTRACTIONS.get(token) \n",
    "                else token for token in tokens]\n",
    "    # remove punctuation\n",
    "#     r = r'[a-z]+'\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    words_clean = [t for t in tokens if t not in stopword_english] # remove all stopwords\n",
    "    words_clean_for_bigrams = [t for t in tokens if t not in ENGLISH_STOPWORDS] # remove stopwords\n",
    "    \n",
    "    unigram_features = bag_of_words(words_clean)\n",
    "    bigram_features = bag_of_ngrams(words_clean_for_bigrams)\n",
    "    \n",
    "    all_features = unigram_features.copy()\n",
    "    for data in bigram_features:\n",
    "        all_features.append(data)\n",
    "    \n",
    "    return words_clean_for_bigrams\n",
    "    #return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'name', 'heriz', 'shreshta', 'not', 'like', 'movie']\n"
     ]
    }
   ],
   "source": [
    "review_data = my_tokenizer(\"hello my name is Heriz Shreshta. i don't like this movie\")\n",
    "print(review_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in positive_review:\n",
    "    orig_reviews.append(review) #positive review add garyo\n",
    "    tokens = my_tokenizer(review) #positive review tokenize garyo\n",
    "    positive_tokenized.append(tokens) #positive token append garyo\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index #token word ko dict bhanayo\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in negative_review:\n",
    "    orig_reviews.append(review)\n",
    "    tokens = my_tokenizer(review)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_index_map): 5252\n"
     ]
    }
   ],
   "source": [
    "print(\"len(word_index_map):\", len(word_index_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # normalize it before setting label\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "# (N x D+1 matrix - keeping them together for now so we can shuffle more easily later\n",
    "data = np.zeros((N, len(word_index_map) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 5253)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "orig_reviews, data = shuffle(orig_reviews, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:,:-1]\n",
    "Y = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,xTest,Ytrain,yTest = train_test_split(\n",
    "                            X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 100\n",
    "alpha = 0.001\n",
    "reg = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    The sigmoid / logistic function.\n",
    "    Args:\n",
    "        z: any real number.\n",
    "    Returns:\n",
    "        A value between O and 1.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-1.0 * z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x,y,weight):\n",
    "    '''\n",
    "    tyo presentation ma gareko hisab le gaar\n",
    "    '''\n",
    "    [n,m] = x.shape\n",
    "    accuracyValue = 0\n",
    "    for i in range(n):\n",
    "        z = np.dot(x,weight)\n",
    "        h = sigmoid(z)\n",
    "    for i in range(len(h)):\n",
    "        if(h[i]>= 0.5 and y[i] == 1):\n",
    "            accuracyValue += 1\n",
    "        elif(h[i] < 0.5 and y[i] == 0):\n",
    "            accuracyValue += 1\n",
    "        else:\n",
    "            accuracyValue += 0\n",
    "    return ((accuracyValue/len(y))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x,y,iteration = 600,lr=0.01,reg = 10,h = lambda a,b: sigmoid(np.dot(a,b))):\n",
    "    \"\"\"\n",
    "    Compute w (Batch gradient descent).\n",
    "    Args:\n",
    "        w: weight vector (numpy.array)\n",
    "        x: documents matrix (numpy.array)\n",
    "        y: output vector (numpy.array)\n",
    "        h: function of x and w\n",
    "    Returns:\n",
    "        The gradient vector (list of float values).\n",
    "    \"\"\"\n",
    "    [n, m] = x.shape\n",
    "    \n",
    "    weight = np.zeros(x.shape[1])\n",
    "    for i in range(iteration):\n",
    "        for j in range(m):\n",
    "            REG = reg * weight[j] / n\n",
    "            weight[j] = weight[j] - lr * ((h(x[i], weight) - y[i]) * x[i,j] - REG)\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h = lambda a,b: sigmoid(np.dot(a,b))\n",
    "weight_data = train(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00060541  0.01096489  0.         -0.00064057 -0.0786936   0.\n",
      "  0.          0.         -0.00522667 -0.00050351]\n"
     ]
    }
   ],
   "source": [
    "print(weight_data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.66666666666667\n"
     ]
    }
   ],
   "source": [
    "result = accuracy(xTest,yTest,weight_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the result in dictionary\n",
    "i = 0\n",
    "for key in word_index_map:\n",
    "    word_index_map[key] = weight_data[i]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0006434160875956348"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index_map['long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.522399915294322\n"
     ]
    }
   ],
   "source": [
    "reviewSen = \"it was a very good movie\"\n",
    "reviewSen = my_tokenizer(reviewSen)\n",
    "value = 0\n",
    "for data in reviewSen:\n",
    "    try:\n",
    "        value += word_index_map[data]\n",
    "    except:\n",
    "        value += 0\n",
    "\n",
    "result = sigmoid(value)\n",
    "print(result * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-21890d5bd0c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpickle_outbu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sentimentResult.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_index_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpickle_outbu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpickle_outbu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_index_map' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
